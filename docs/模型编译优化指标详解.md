# AI模型编译优化指标详解
## Model Compile中的关键性能指标

---

## 目录

1. [什么是模型编译](#什么是模型编译)
2. [核心优化指标体系](#核心优化指标体系)
3. [性能指标](#性能指标)
4. [资源指标](#资源指标)
5. [精度指标](#精度指标)
6. [能耗指标](#能耗指标)
7. [可靠性指标](#可靠性指标)
8. [成本指标](#成本指标)
9. [实战案例](#实战案例)
10. [指标权衡与选择](#指标权衡与选择)

---

## 什么是模型编译

### 基本概念

**模型编译（Model Compilation）** 是将训练好的深度学习模型转换、优化并部署到目标硬件上的过程。

```
训练好的模型
    ↓
模型编译优化
    ├─ 图优化
    ├─ 算子融合
    ├─ 内存优化
    ├─ 量化压缩
    └─ 硬件适配
    ↓
优化后的模型
```

### 为什么需要优化？

**问题：**
- 训练模型通常很大（几百MB到几GB）
- 推理速度慢（无法实时响应）
- 内存占用高（移动设备无法运行）
- 功耗大（设备发热、耗电快）

**解决方案：**
通过模型编译优化，在**多个维度**平衡性能。

---

## 核心优化指标体系

### 指标金字塔

<p align="center">
  <img src="images/model_metrics/impossible_triangle.png" width="600" alt="性能不可能三角">
</p>

*图1: 模型优化不可能三角 - 高精度、低延迟、低资源无法同时满足，需要根据场景权衡*

### 指标分类

| 类别 | 指标 | 重要性 | 典型场景 |
|------|------|--------|---------|
| **性能** | 延迟、吞吐量、FPS | ⭐⭐⭐⭐⭐ | 实时系统 |
| **资源** | 内存、存储、算力 | ⭐⭐⭐⭐⭐ | 嵌入式设备 |
| **精度** | 准确率、Loss | ⭐⭐⭐⭐⭐ | 所有场景 |
| **能耗** | 功耗、电池寿命 | ⭐⭐⭐⭐ | 移动设备 |
| **可靠性** | 稳定性、错误率 | ⭐⭐⭐⭐ | 生产环境 |
| **成本** | 硬件成本、云费用 | ⭐⭐⭐ | 商业应用 |

---

---

## 性能指标

<p align="center">
  <img src="images/model_metrics/latency_requirements.png" width="700" alt="延迟要求对比">
</p>

*图2: 不同应用场景的推理延迟要求对比*

### 1. 推理延迟（Latency）

**定义：**  
单次推理所需的时间（从输入到输出）。

**单位：** 毫秒（ms）、微秒（μs）

**重要性：** ⭐⭐⭐⭐⭐

**应用场景：**
- 实时视频处理
- 语音识别
- 自动驾驶
- 游戏AI

**衡量标准：**
```
P50延迟：50%的请求在此时间内完成
P90延迟：90%的请求在此时间内完成
P99延迟：99%的请求在此时间内完成（关注长尾）
```

**优化目标：**
```
实时应用：    < 30ms   （33 FPS）
交互应用：    < 100ms
在线服务：    < 200ms
批处理：      不限
```

**实例代码：**
```python
import time
import numpy as np

# 测量推理延迟
def measure_latency(model, input_data, num_runs=100):
    latencies = []
    
    for _ in range(num_runs):
        start = time.perf_counter()
        output = model(input_data)
        end = time.perf_counter()
        
        latencies.append((end - start) * 1000)  # 转为毫秒
    
    return {
        'mean': np.mean(latencies),
        'p50': np.percentile(latencies, 50),
        'p90': np.percentile(latencies, 90),
        'p99': np.percentile(latencies, 99),
        'min': np.min(latencies),
        'max': np.max(latencies)
    }

# 示例
stats = measure_latency(model, input_tensor)
print(f"平均延迟: {stats['mean']:.2f} ms")
print(f"P99延迟: {stats['p99']:.2f} ms")
```

### 2. 吞吐量（Throughput）

**定义：**  
单位时间内可以处理的样本数量。

**单位：** 
- QPS（Queries Per Second，每秒查询数）
- FPS（Frames Per Second，每秒帧数）
- Samples/second（样本/秒）

**重要性：** ⭐⭐⭐⭐⭐

**应用场景：**
- 云端批量推理
- 视频分析
- 大规模数据处理

**计算公式：**
```
吞吐量 = Batch Size / 推理时间
```

**优化目标：**
```
视频处理：     30+ FPS
图像分类：     1000+ QPS
文本生成：     100+ tokens/s
```

**实例代码：**
```python
def measure_throughput(model, input_data, duration=10):
    """
    测量吞吐量
    """
    start = time.time()
    count = 0
    
    while time.time() - start < duration:
        output = model(input_data)
        count += input_data.shape[0]  # batch size
    
    elapsed = time.time() - start
    throughput = count / elapsed
    
    return throughput

# 示例
batch_size = 32
input_tensor = torch.randn(batch_size, 3, 224, 224)
qps = measure_throughput(model, input_tensor)
print(f"吞吐量: {qps:.2f} samples/s")
```

### 3. FLOPs（浮点运算次数）

**定义：**  
Floating Point Operations - 模型推理需要的浮点计算次数。

**单位：** 
- FLOPs（浮点运算次数）
- GFLOPs（十亿次浮点运算）
- TFLOPs（万亿次浮点运算）

**重要性：** ⭐⭐⭐⭐

**用途：**
- 评估模型复杂度
- 预测推理速度
- 对比不同架构

**典型值：**
```
MobileNetV2:     ~0.3 GFLOPs
ResNet-50:       ~4 GFLOPs
VGG-16:          ~15 GFLOPs
Transformer:     几十到几百 GFLOPs
```

**计算示例：**
```python
from thop import profile
import torch

# 计算FLOPs
input_tensor = torch.randn(1, 3, 224, 224)
flops, params = profile(model, inputs=(input_tensor,))

print(f"FLOPs: {flops / 1e9:.2f} G")
print(f"参数量: {params / 1e6:.2f} M")
```

### 4. 帧率（FPS）

**定义：**  
Frames Per Second - 视频处理中每秒处理的帧数。

**重要性：** ⭐⭐⭐⭐⭐（视频应用）

**标准：**
```
电影：         24 FPS
电视：         30 FPS
游戏/VR：      60+ FPS
实时监控：     15-30 FPS
```

---

## 资源指标

### 1. 内存占用（Memory Usage）

**定义：**  
模型运行时占用的内存空间。

**重要性：** ⭐⭐⭐⭐⭐

**分类：**

**A. 模型权重内存**
```python
# 计算模型参数大小
def model_size_mb(model):
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

print(f"模型大小: {model_size_mb(model):.2f} MB")
```

**B. 激活值内存（Activation Memory）**
- 前向传播时的中间结果
- Batch size越大，占用越多

**C. 峰值内存（Peak Memory）**
```python
import torch

# 监控GPU内存
torch.cuda.reset_peak_memory_stats()
output = model(input_tensor)
peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
print(f"峰值内存: {peak_memory:.2f} MB")
```

**优化目标：**
```
移动设备：      < 100 MB
嵌入式：        < 50 MB
服务器：        < 4 GB
边缘设备：      < 500 MB
```

### 2. 模型大小（Model Size）

**定义：**  
模型文件的磁盘存储大小。

**重要性：** ⭐⭐⭐⭐

**计算：**
```
模型大小 = 参数数量 × 每个参数的字节数

FP32: 4 bytes/param
FP16: 2 bytes/param
INT8: 1 byte/param
```

**示例：**
```
ResNet-50:
- 参数量: 25.6M
- FP32: 25.6M × 4 = 102.4 MB
- FP16: 25.6M × 2 = 51.2 MB
- INT8: 25.6M × 1 = 25.6 MB
```

**优化技术：**
- **量化（Quantization）**: FP32 → INT8，减小4倍
- **剪枝（Pruning）**: 去除不重要的连接
- **蒸馏（Distillation）**: 用小模型学习大模型
- **压缩（Compression）**: 权重共享、霍夫曼编码

### 3. 参数量（Parameters）

**定义：**  
模型中可学习参数的数量。

**单位：** 
- K（千）
- M（百万）
- B（十亿）

**典型值：**
```
MobileNetV2:      3.5M
ResNet-50:        25.6M
BERT-base:        110M
GPT-3:            175B
```

**计算代码：**
```python
def count_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        'total': total,
        'trainable': trainable,
        'non_trainable': total - trainable
    }

params = count_parameters(model)
print(f"总参数: {params['total'] / 1e6:.2f} M")
```

### 4. 算力需求（Compute Requirements）

**定义：**  
硬件算力要求，通常以TOPs衡量。

**单位：**
- GOPS (Giga Operations Per Second)
- TOPS (Tera Operations Per Second)

**硬件对照：**
```
CPU (Intel i7):          ~0.5 TOPS
移动GPU (Mali):          ~1-2 TOPS
边缘TPU (Coral):         ~4 TOPS
高端GPU (RTX 4090):      ~1300 TOPS
数据中心GPU (A100):      ~2500 TOPS
```

---

## 精度指标

### 1. 准确率（Accuracy）

**定义：**  
模型预测正确的比例。

**重要性：** ⭐⭐⭐⭐⭐

**计算公式：**
```
Accuracy = 正确预测数 / 总样本数
```

**代码示例：**
```python
def calculate_accuracy(model, dataloader):
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in dataloader:
            output = model(data)
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    
    accuracy = correct / total
    return accuracy

acc = calculate_accuracy(model, test_loader)
print(f"准确率: {acc * 100:.2f}%")
```

### 2. 精度损失（Accuracy Drop）

**定义：**  
优化后模型相比原始模型的准确率下降。

**重要性：** ⭐⭐⭐⭐⭐

**可接受范围：**
```
量化 (INT8):       < 1% drop
剪枝 (50%):        < 2% drop
蒸馏:              < 3% drop
极限压缩:          < 5% drop
```

**计算：**
```python
original_acc = 0.945
optimized_acc = 0.938
drop = (original_acc - optimized_acc) * 100
print(f"精度损失: {drop:.2f}%")
```

### 3. mAP（平均精度）

**定义：**  
Mean Average Precision - 目标检测中的关键指标。

**重要性：** ⭐⭐⭐⭐⭐（检测任务）

**计算：**
涉及IoU、Precision、Recall等多个子指标。

### 4. BLEU / ROUGE（文本指标）

**用于：** 机器翻译、文本生成

**BLEU（双语评估替换）：**
- 评估翻译质量
- 范围：0-100，越高越好

**ROUGE（面向摘要的评估）：**
- 评估摘要质量
- ROUGE-1, ROUGE-2, ROUGE-L

---

## 能耗指标

### 1. 功耗（Power Consumption）

**定义：**  
模型推理时的能量消耗速率。

**单位：** 瓦特（W）、毫瓦（mW）

**重要性：** ⭐⭐⭐⭐（移动/边缘设备）

**测量：**
```python
# 使用NVIDIA工具测量GPU功耗
import subprocess

def measure_gpu_power():
    result = subprocess.run(
        ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'],
        capture_output=True, text=True
    )
    power = float(result.stdout.strip())
    return power

# 测量推理过程功耗
powers = []
for _ in range(100):
    power = measure_gpu_power()
    output = model(input_tensor)
    powers.append(power)

avg_power = np.mean(powers)
print(f"平均功耗: {avg_power:.2f} W")
```

**典型值：**
```
手机芯片：       1-5W
边缘设备：       5-15W
桌面GPU：        100-350W
数据中心GPU：    250-500W
```

### 2. 能效比（Energy Efficiency）

**定义：**  
单位能量可以完成的推理次数。

**计算：**
```
能效比 = 吞吐量 / 功耗
单位: Inferences per Watt (inf/W)
```

**示例：**
```python
throughput = 1000  # samples/s
power = 150  # W
efficiency = throughput / power
print(f"能效比: {efficiency:.2f} inf/W")
```

### 3. 单次推理能耗

**定义：**  
完成一次推理所消耗的能量。

**计算：**
```
单次能耗 = 功耗 × 推理时间
单位: 焦耳（J）或毫焦（mJ）
```

**示例：**
```python
power = 10  # W
latency = 0.05  # 50ms
energy = power * latency
print(f"单次推理能耗: {energy * 1000:.2f} mJ")
```

### 4. 电池寿命影响

**定义：**  
模型运行对设备电池寿命的影响。

**计算：**
```
电池可用次数 = 电池容量(mAh) / 单次推理耗电(mA·s)
```

---

## 可靠性指标

### 1. 推理稳定性

**定义：**  
多次推理结果的一致性。

**测量：**
```python
def test_stability(model, input_data, num_runs=100):
    outputs = []
    for _ in range(num_runs):
        output = model(input_data)
        outputs.append(output.detach().cpu().numpy())
    
    # 计算标准差
    std = np.std(outputs, axis=0).mean()
    return std

stability = test_stability(model, input_tensor)
print(f"输出标准差: {stability:.6f}")
```

### 2. 数值精度

**问题：** 量化可能导致数值溢出或下溢

**检查：**
```python
# 检查输出是否包含NaN或Inf
def check_numerical_stability(model, test_loader):
    for data, _ in test_loader:
        output = model(data)
        
        if torch.isnan(output).any():
            print("⚠️ 检测到NaN")
            return False
        
        if torch.isinf(output).any():
            print("⚠️ 检测到Inf")
            return False
    
    return True
```

### 3. 错误率（Error Rate）

**定义：**  
推理失败或崩溃的比率。

**目标：** < 0.01%（99.99%可靠性）

---

## 成本指标

### 1. 硬件成本

**考虑因素：**
- 芯片价格
- 内存/存储成本
- 外围电路成本

**示例：**
```
边缘TPU (Coral):     $25
Jetson Nano:         $99
Jetson Xavier NX:    $399
Tesla T4:            $2,000
A100 GPU:            $10,000+
```

### 2. 云服务费用

**计算：**
```
费用 = (推理次数 / 吞吐量) × 小时费率
```

**AWS示例：**
```
g4dn.xlarge (T4 GPU):    $0.526/hour
p3.2xlarge (V100):       $3.06/hour
p4d.24xlarge (A100):     $32.77/hour
```

### 3. 研发成本

**包括：**
- 模型训练时间
- 优化调试时间
- 工程师成本

---

---

## 实战案例

<p align="center">
  <img src="images/model_metrics/optimization_techniques.png" width="800" alt="优化技术效果对比">
</p>

*图3: 量化、剪枝、蒸馏等优化技术的效果对比*

<p align="center">
  <img src="images/model_metrics/quantization_comparison.png" width="700" alt="量化精度对比">
</p>

*图4: FP32、FP16、INT8不同精度的模型大小与准确率对比*

<p align="center">
  <img src="images/model_metrics/throughput_vs_latency.png" width="700" alt="吞吐量vs延迟权衡">
</p>

*图5: 吞吐量与延迟的权衡关系，不同batch size的性能表现*

### 案例1：移动端图像分类

**场景：** 手机APP实时识别物体

**优化目标：**
```
准确率:    > 90%
延迟:      < 50ms (20 FPS)
模型大小:  < 10MB
功耗:      < 2W
```

**方案：**
```python
# 1. 选择轻量级架构
model = MobileNetV3_Small()

# 2. 量化为INT8
import torch.quantization as quantization

model.qconfig = quantization.get_default_qconfig('qnnpack')
model_prepared = quantization.prepare(model)
# ... 校准 ...
model_quantized = quantization.convert(model_prepared)

# 3. 测试指标
print(f"模型大小: {model_size_mb(model_quantized):.2f} MB")  # ~5MB
print(f"推理延迟: {measure_latency(model_quantized, input)['mean']:.2f} ms")  # ~30ms
print(f"准确率: {calculate_accuracy(model_quantized, test_loader) * 100:.2f}%")  # 91%
```

### 案例2：云端批量推理

**场景：** 每天处理100万张图片

**优化目标：**
```
准确率:    > 95%
吞吐量:    > 500 QPS
成本:      < $100/day
```

**方案：**
```python
# 1. 使用大batch + TensorRT优化
import torch_tensorrt

compiled_model = torch_tensorrt.compile(
    model,
    inputs=[torch_tensorrt.Input((32, 3, 224, 224))],  # batch=32
    enabled_precisions={torch.float16},  # FP16
)

# 2. 动态batching
from collections import deque

class DynamicBatcher:
    def __init__(self, model, max_batch=32, timeout=0.01):
        self.model = model
        self.max_batch = max_batch
        self.timeout = timeout
        self.queue = deque()
    
    def infer(self, input):
        self.queue.append(input)
        
        if len(self.queue) >= self.max_batch:
            batch = torch.cat(list(self.queue)[:self.max_batch])
            self.queue = deque(list(self.queue)[self.max_batch:])
            return self.model(batch)

# 3. 性能测试
throughput = measure_throughput(compiled_model, large_batch_input)
print(f"吞吐量: {throughput:.2f} QPS")  # ~650 QPS
```

### 案例3：边缘设备实时检测

**场景：** Jetson Nano上运行目标检测

**优化目标：**
```
mAP:       > 0.45
FPS:       > 15
功耗:      < 10W
内存:      < 2GB
```

**方案：**
```python
# 1. 使用YOLO-Nano或MobileNet-SSD
from models import YOLONano

model = YOLONano(num_classes=80)

# 2. TensorRT优化
import tensorrt as trt

# 导出ONNX
torch.onnx.export(model, dummy_input, "model.onnx")

# 转换为TensorRT
# trtexec --onnx=model.onnx --saveEngine=model.trt --fp16

# 3. 性能监控
import psutil

def monitor_resources():
    cpu_percent = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    
    return {
        'cpu': cpu_percent,
        'memory_mb': memory.used / 1024 / 1024,
        'memory_percent': memory.percent
    }

stats = monitor_resources()
print(f"内存占用: {stats['memory_mb']:.2f} MB")
print(f"CPU使用率: {stats['cpu']:.2f}%")
```

---

## 指标权衡与选择

### 不可能三角

<p align="center">
  <img src="images/model_metrics/impossible_triangle.png" width="600" alt="性能不可能三角">
</p>

**无法同时满足所有要求，需要权衡！**

### 场景优先级

<p align="center">
  <img src="images/model_metrics/scenario_priorities.png" width="700" alt="场景优先级热力图">
</p>

*图6: 不同应用场景对各项指标的优先级热力图*

#### 场景A：实时视频分析
```
优先级排序：
1. 延迟（< 33ms for 30FPS）     ⭐⭐⭐⭐⭐
2. 准确率（> 85%）               ⭐⭐⭐⭐
3. 功耗（< 15W）                 ⭐⭐⭐
4. 模型大小                      ⭐⭐
```

#### 场景B：移动APP
```
优先级排序：
1. 模型大小（< 10MB）            ⭐⭐⭐⭐⭐
2. 延迟（< 100ms）               ⭐⭐⭐⭐
3. 功耗（< 3W）                  ⭐⭐⭐⭐
4. 准确率（> 80%）               ⭐⭐⭐
```

#### 场景C：云端服务
```
优先级排序：
1. 准确率（> 95%）               ⭐⭐⭐⭐⭐
2. 吞吐量（> 1000 QPS）          ⭐⭐⭐⭐⭐
3. 成本（< $500/day）            ⭐⭐⭐⭐
4. 延迟（< 200ms）               ⭐⭐⭐
```

#### 场景D：嵌入式/IoT
```
优先级排序：
1. 功耗（< 1W）                  ⭐⭐⭐⭐⭐
2. 模型大小（< 5MB）             ⭐⭐⭐⭐⭐
3. 内存（< 100MB）               ⭐⭐⭐⭐
4. 准确率（> 75%）               ⭐⭐⭐
```

### 决策树

```
需要实时响应？
├─ 是 → 优先优化延迟
│       ├─ 移动设备？
│       │   ├─ 是 → 量化 + 剪枝 + 轻量架构
│       │   └─ 否 → TensorRT + FP16
│       └─ 可接受精度损失吗？
│           ├─ 是 → INT8量化
│           └─ 否 → FP16量化
│
└─ 否 → 优先保证准确率
        ├─ 批量处理？
        │   ├─ 是 → 增大batch + GPU并行
        │   └─ 否 → 使用大模型
        └─ 有成本限制？
            ├─ 是 → 云GPU + 动态batching
            └─ 否 → 多卡并行 + 最优模型
```

### 优化策略矩阵

| 优化技术 | 准确率影响 | 速度提升 | 大小压缩 | 适用场景 |
|---------|-----------|---------|---------|---------|
| **量化INT8** | -1~2% | 2-4x | 4x | 移动/边缘 |
| **剪枝50%** | -2~5% | 1.5-2x | 2x | 所有场景 |
| **蒸馏** | -3~5% | 5-10x | 10x+ | 需要极致性能 |
| **TensorRT** | 0% | 2-5x | 1x | NVIDIA GPU |
| **算子融合** | 0% | 1.2-1.5x | 1x | 所有场景 |

### 实战建议

**第1步：明确约束**
```python
constraints = {
    'latency_ms': 50,          # 硬约束
    'model_size_mb': 20,       # 硬约束
    'accuracy_min': 0.90,      # 软约束
    'power_w': 5               # 软约束
}
```

**第2步：基准测试**
```python
def benchmark_model(model):
    return {
        'accuracy': test_accuracy(model),
        'latency': measure_latency(model),
        'model_size': model_size_mb(model),
        'throughput': measure_throughput(model)
    }

baseline = benchmark_model(original_model)
```

**第3步：逐步优化**
```python
# 1. 量化
quantized_model = quantize_model(original_model)
metrics_1 = benchmark_model(quantized_model)

# 2. 如果还不满足，继续剪枝
if metrics_1['latency'] > constraints['latency_ms']:
    pruned_model = prune_model(quantized_model, ratio=0.3)
    metrics_2 = benchmark_model(pruned_model)

# 3. 如果精度下降太多，用蒸馏恢复
if metrics_2['accuracy'] < constraints['accuracy_min']:
    distilled_model = distill_model(pruned_model, teacher=original_model)
    final_metrics = benchmark_model(distilled_model)
```

**第4步：验证部署**
```python
def validate_deployment(model, constraints):
    metrics = benchmark_model(model)
    
    # 检查所有约束
    checks = {
        'latency': metrics['latency'] <= constraints['latency_ms'],
        'size': metrics['model_size'] <= constraints['model_size_mb'],
        'accuracy': metrics['accuracy'] >= constraints['accuracy_min']
    }
    
    if all(checks.values()):
        print("✅ 所有约束满足，可以部署")
        return True
    else:
        print("❌ 以下约束未满足:")
        for key, passed in checks.items():
            if not passed:
                print(f"  - {key}: {metrics[key]} vs {constraints.get(key)}")
        return False
```

---

## 总结

### 核心要点

**1. 没有万能的优化方案**
- 不同场景优先级不同
- 需要根据实际需求权衡

**2. 关键指标**
```
性能: 延迟、吞吐量、FPS
资源: 内存、模型大小、算力
精度: 准确率、精度损失
能耗: 功耗、能效比
可靠性: 稳定性、错误率
成本: 硬件、云服务费用
```

**3. 优化三板斧**
- **量化**: 最常用，效果好
- **剪枝**: 去除冗余
- **蒸馏**: 精度损失小

**4. 评估流程**
```
1. 定义约束（硬性要求）
2. 基准测试（原始模型）
3. 逐步优化（量化→剪枝→蒸馏）
4. 权衡取舍（牺牲次要指标）
5. 部署验证（实际环境测试）
```

### 快速查找表

**我的场景是什么？**
| 场景 | 优先指标 | 推荐方案 |
|------|---------|---------|
| 手机APP | 大小、功耗 | MobileNet + INT8 |
| 实时视频 | 延迟、FPS | TensorRT + FP16 |
| 云端批处理 | 吞吐量、成本 | 大batch + FP16 |
| 边缘设备 | 功耗、内存 | Nano模型 + INT8 |
| IoT | 极低功耗 | TinyML + 定点运算 |

### 进阶学习

**推荐工具：**
1. **TensorRT** - NVIDIA GPU优化
2. **ONNX Runtime** - 跨平台推理
3. **OpenVINO** - Intel硬件优化
4. **TFLite** - 移动端部署
5. **PyTorch Mobile** - 移动端框架

**推荐阅读：**
1. TensorRT文档
2. 《Efficient Deep Learning》
3. 模型压缩综述论文

---

*本文档帮助您全面理解AI模型编译优化的各个维度*  
*根据实际场景选择合适的优化策略*  
*在性能、精度、资源之间找到最佳平衡点*
